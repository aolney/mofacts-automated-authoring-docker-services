{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Data loader\n",
    "\n",
    "The purpose of this notebook is to load data for the long form qa model that is used to generate elaborated feedback.\n",
    "\n",
    "Ideally the data will come from a customized source that is close to your domain.\n",
    "For example, if your domain is biology, you might use an OpenStax biology textbook.\n",
    "\n",
    "However, you can also use Wikipedia, which is provided by HuggingFace.\n",
    "\n",
    "The code below closely follows [this implementation of long form question answering.](https://yjernite.github.io/lfqa.html)\n",
    "\n",
    "## Use\n",
    "\n",
    "- Locally installed via conda `longformqa` environment, i.e. run `conda env create -f environment.yml`\n",
    "- Make sure the docker service is running by executing `run.sh` in the top level directory. It may take 10-30 minutes for it to complete initialization.\n",
    "- If you don't understand what a cell is doing, just run it. The initial cells set things up, and the last couple of cells actually load the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Installation requirements\n",
    "\n",
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: elasticsearch in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (8.6.2)\n",
      "Requirement already satisfied: elastic-transport<9,>=8 in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from elasticsearch) (8.4.0)\n",
      "Requirement already satisfied: certifi in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from elastic-transport<9,>=8->elasticsearch) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<2,>=1.26.2 in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from elastic-transport<9,>=8->elasticsearch) (1.26.15)\n",
      "Collecting faiss_gpu\n",
      "  Downloading faiss_gpu-1.7.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 85.5 MB 217 kB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: faiss-gpu\n",
      "Successfully installed faiss-gpu-1.7.2\n",
      "Collecting nlp\n",
      "  Downloading nlp-0.4.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7 MB 878 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: dill in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from nlp) (0.3.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from nlp) (4.63.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from nlp) (2.24.0)\n",
      "Requirement already satisfied: numpy in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from nlp) (1.19.1)\n",
      "Requirement already satisfied: pandas in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from nlp) (1.0.5)\n",
      "Requirement already satisfied: pyarrow>=0.16.0 in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from nlp) (3.0.0)\n",
      "Requirement already satisfied: filelock in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from nlp) (3.6.0)\n",
      "Requirement already satisfied: xxhash in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from nlp) (1.4.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from requests>=2.19.0->nlp) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from requests>=2.19.0->nlp) (2021.10.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from requests>=2.19.0->nlp) (3.0.4)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
      "\u001b[K     |████████████████████████████████| 127 kB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from pandas->nlp) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from pandas->nlp) (2020.1)\n",
      "Requirement already satisfied: six>=1.5 in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from python-dateutil>=2.6.1->pandas->nlp) (1.15.0)\n",
      "\u001b[31mERROR: elastic-transport 8.4.0 has requirement urllib3<2,>=1.26.2, but you'll have urllib3 1.25.11 which is incompatible.\u001b[0m\n",
      "Installing collected packages: nlp, urllib3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.15\n",
      "    Uninstalling urllib3-1.26.15:\n",
      "      Successfully uninstalled urllib3-1.26.15\n",
      "Successfully installed nlp-0.4.0 urllib3-1.25.11\n",
      "Requirement already satisfied: transformers in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (3.0.2)\n",
      "Requirement already satisfied: sacremoses in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: numpy in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from transformers) (1.19.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from transformers) (2021.3.17)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from transformers) (0.1.95)\n",
      "Requirement already satisfied: packaging in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from transformers) (4.63.0)\n",
      "Requirement already satisfied: tokenizers==0.8.1.rc1 in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from transformers) (0.8.1rc1)\n",
      "Requirement already satisfied: filelock in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: requests in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: joblib in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: six in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from requests->transformers) (3.0.4)\n",
      "Collecting apache-beam\n",
      "  Downloading apache_beam-2.46.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.8 MB 1.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pydot<2,>=1.2.0 in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from apache-beam) (1.2.4)\n",
      "Requirement already satisfied: pyarrow<10.0.0,>=3.0.0 in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from apache-beam) (3.0.0)\n",
      "Collecting cloudpickle~=2.2.1\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: numpy<1.25.0,>=1.14.3 in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from apache-beam) (1.19.1)\n",
      "Collecting crcmod<2.0,>=1.7\n",
      "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
      "\u001b[K     |████████████████████████████████| 89 kB 1.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting hdfs<3.0.0,>=2.1.0\n",
      "  Downloading hdfs-2.7.0-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.0 in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from apache-beam) (3.7.4.3)\n",
      "Requirement already satisfied: pytz>=2018.3 in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from apache-beam) (2020.1)\n",
      "Requirement already satisfied: protobuf<4,>3.12.2 in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from apache-beam) (3.15.6)\n",
      "Collecting orjson<4.0\n",
      "  Downloading orjson-3.8.9-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (274 kB)\n",
      "\u001b[K     |████████████████████████████████| 274 kB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3,>=2.8.0 in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from apache-beam) (2.8.1)\n",
      "Collecting httplib2<0.22.0,>=0.8\n",
      "  Downloading httplib2-0.21.0-py3-none-any.whl (96 kB)\n",
      "\u001b[K     |████████████████████████████████| 96 kB 1.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fastavro<2,>=0.23.6\n",
      "  Downloading fastavro-1.7.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.7 MB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting proto-plus<2,>=1.7.1\n",
      "  Downloading proto_plus-1.22.2-py3-none-any.whl (47 kB)\n",
      "\u001b[K     |████████████████████████████████| 47 kB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting zstandard<1,>=0.18.0\n",
      "  Downloading zstandard-0.20.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.6 MB 1.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex>=2020.6.8 in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from apache-beam) (2021.3.17)\n",
      "Collecting pymongo<4.0.0,>=3.8.0\n",
      "  Downloading pymongo-3.13.0-cp38-cp38-manylinux2014_x86_64.whl (545 kB)\n",
      "\u001b[K     |████████████████████████████████| 545 kB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dill<0.3.2,>=0.3.1.1\n",
      "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
      "\u001b[K     |████████████████████████████████| 151 kB 3.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: fasteners<1.0,>=0.3 in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from apache-beam) (0.15)\n",
      "Requirement already satisfied: grpcio!=1.48.0,<2,>=1.33.1 in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from apache-beam) (1.36.1)\n",
      "Collecting objsize<0.7.0,>=0.6.1\n",
      "  Downloading objsize-0.6.1-py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from apache-beam) (2.24.0)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from pydot<2,>=1.2.0->apache-beam) (2.4.7)\n",
      "Collecting docopt\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam) (1.15.0)\n",
      "Requirement already satisfied: monotonic>=0.1 in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from fasteners<1.0,>=0.3->apache-beam) (1.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from requests<3.0.0,>=2.24.0->apache-beam) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from requests<3.0.0,>=2.24.0->apache-beam) (2021.10.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from requests<3.0.0,>=2.24.0->apache-beam) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /z/aolney/software/miniconda3/envs/paraphrase-t5/lib/python3.8/site-packages (from requests<3.0.0,>=2.24.0->apache-beam) (2.10)\n",
      "Building wheels for collected packages: crcmod, dill, docopt\n",
      "  Building wheel for crcmod (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for crcmod: filename=crcmod-1.7-cp38-cp38-linux_x86_64.whl size=31121 sha256=b1bccba176a40b2b57a48265687330e62bc4e0daf692abb81c342e71d405f9c1\n",
      "  Stored in directory: /home/aolney/.cache/pip/wheels/ca/5a/02/f3acf982a026f3319fb3e798a8dca2d48fafee7761788562e9\n",
      "  Building wheel for dill (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78530 sha256=1ca49dbe14ac27ec29a7b228d6253eb2b232c58a4544c2893059b4f550915f18\n",
      "  Stored in directory: /home/aolney/.cache/pip/wheels/07/35/78/e9004fa30578734db7f10e7a211605f3f0778d2bdde38a239d\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13704 sha256=97a8adb0a0d71d4940d5412c2de0e6e3c838db23871769975c17c366793b0af4\n",
      "  Stored in directory: /home/aolney/.cache/pip/wheels/56/ea/58/ead137b087d9e326852a851351d1debf4ada529b6ac0ec4e8c\n",
      "Successfully built crcmod dill docopt\n",
      "\u001b[31mERROR: multiprocess 0.70.12.2 has requirement dill>=0.3.4, but you'll have dill 0.3.1.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: proto-plus 1.22.2 has requirement protobuf<5.0.0dev,>=3.19.0, but you'll have protobuf 3.15.6 which is incompatible.\u001b[0m\n",
      "Installing collected packages: cloudpickle, crcmod, docopt, hdfs, orjson, httplib2, fastavro, proto-plus, zstandard, pymongo, dill, objsize, apache-beam\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.4\n",
      "    Uninstalling dill-0.3.4:\n",
      "      Successfully uninstalled dill-0.3.4\n",
      "Successfully installed apache-beam-2.46.0 cloudpickle-2.2.1 crcmod-1.7 dill-0.3.1.1 docopt-0.6.2 fastavro-1.7.3 hdfs-2.7.0 httplib2-0.21.0 objsize-0.6.1 orjson-3.8.9 proto-plus-1.22.2 pymongo-3.13.0 zstandard-0.20.0\n"
     ]
    }
   ],
   "source": [
    "! pip install elasticsearch==7.9.1\n",
    "! pip install faiss_gpu\n",
    "! pip install nlp\n",
    "! pip install transformers\n",
    "! pip install apache-beam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "bash"
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "Python3"
   },
   "outputs": [],
   "source": [
    "import nlp\n",
    "eli5 = nlp.load_dataset('eli5')\n",
    "wiki40b_snippets = nlp.load_dataset('wiki_snippets', name='wiki40b_en_100_0')['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Python3"
   },
   "source": [
    "## Utilities\n",
    "\n",
    "Copied from this [utilities script](https://github.com/huggingface/notebooks/blob/master/longform-qa/lfqa_utils.py), but with adjustments for warnings/errors **and with custom code for textbooks.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "kernel": "Python3"
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import math\n",
    "import os  # noqa: F401\n",
    "from random import choice, randint\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "import faiss  # noqa: F401\n",
    "import nlp  # noqa: F401\n",
    "import pandas as pd\n",
    "from elasticsearch import Elasticsearch  # noqa: F401\n",
    "from elasticsearch.helpers import bulk, streaming_bulk  # noqa: F401\n",
    "from transformers import AdamW, AutoModel, AutoModelForSeq2SeqLM, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "\n",
    "###############\n",
    "# Sparse index\n",
    "###############\n",
    "\n",
    "# passages_dest is an array of appropriate objects rather than huggingface dataset\n",
    "def make_es_index_snippets_textbook( es_client, passages_dset, index_name=\"textbook_snippets_100w\"):\n",
    "    index_config = {\n",
    "        \"settings\": {\n",
    "            \"number_of_shards\": 1,\n",
    "            \"analysis\": {\"analyzer\": {\"stop_standard\": {\"type\": \"standard\", \" stopwords\": \"_english_\"}}},\n",
    "        },\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"article_title\": {\"type\": \"text\", \"analyzer\": \"standard\", \"similarity\": \"BM25\"},\n",
    "                \"section_title\": {\"type\": \"text\", \"analyzer\": \"standard\", \"similarity\": \"BM25\"},\n",
    "                \"passage_text\": {\"type\": \"text\", \"analyzer\": \"standard\", \"similarity\": \"BM25\"},\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "    es_client.indices.create(index=index_name, body=index_config)\n",
    "    number_of_docs = len( passages_dset )\n",
    "    progress = tqdm(unit=\"docs\", total=number_of_docs)\n",
    "    successes = 0\n",
    "\n",
    "    def passage_generator():\n",
    "        for passage in passages_dset:\n",
    "            yield passage\n",
    "\n",
    "    # create the ES index\n",
    "    for ok, action in streaming_bulk(client=es_client, index=index_name, actions=passage_generator(),):\n",
    "        progress.update(1)\n",
    "        successes += ok\n",
    "    print(\"Indexed %d documents\" % (successes,))\n",
    "\n",
    "    \n",
    "def make_es_index_snippets(es_client, passages_dset, index_name=\"english_wiki_kilt_snippets_100w\"):\n",
    "    index_config = {\n",
    "        \"settings\": {\n",
    "            \"number_of_shards\": 1,\n",
    "            \"analysis\": {\"analyzer\": {\"stop_standard\": {\"type\": \"standard\", \" stopwords\": \"_english_\"}}},\n",
    "        },\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"article_title\": {\"type\": \"text\", \"analyzer\": \"standard\", \"similarity\": \"BM25\"},\n",
    "                \"section_title\": {\"type\": \"text\", \"analyzer\": \"standard\", \"similarity\": \"BM25\"},\n",
    "                \"passage_text\": {\"type\": \"text\", \"analyzer\": \"standard\", \"similarity\": \"BM25\"},\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "    es_client.indices.create(index=index_name, body=index_config)\n",
    "    number_of_docs = passages_dset.num_rows\n",
    "    progress = tqdm(unit=\"docs\", total=number_of_docs)\n",
    "    successes = 0\n",
    "\n",
    "    def passage_generator():\n",
    "        for passage in passages_dset:\n",
    "            yield passage\n",
    "\n",
    "    # create the ES index\n",
    "    for ok, action in streaming_bulk(client=es_client, index=index_name, actions=passage_generator(),):\n",
    "        progress.update(1)\n",
    "        successes += ok\n",
    "    print(\"Indexed %d documents\" % (successes,))\n",
    "\n",
    "#Identical to original query except with different index_name default\n",
    "def query_es_index_textbook(question, es_client, index_name=\"textbook_snippets_100w\", n_results=10, min_length=20):\n",
    "    q = question.lower()\n",
    "    banned = [\"how\", \"why\", \"what\", \"where\", \"which\", \"do\", \"does\", \"is\", \"?\", \"eli5\", \"eli5:\"]\n",
    "    q = \" \".join([w for w in q.split() if w not in banned])\n",
    "    response = es_client.search(\n",
    "        index=index_name,\n",
    "        body={\n",
    "            \"query\": {\n",
    "                \"multi_match\": {\n",
    "                    \"query\": q,\n",
    "                    \"fields\": [\"article_title\", \"section_title\", \"passage_text^2\"],\n",
    "                    \"type\": \"cross_fields\",\n",
    "                }\n",
    "            },\n",
    "            \"size\": 2 * n_results,\n",
    "        },\n",
    "    )\n",
    "    hits = response[\"hits\"][\"hits\"]\n",
    "    support_doc = \"<P> \" + \" <P> \".join([hit[\"_source\"][\"passage_text\"] for hit in hits])\n",
    "    res_list = [dict([(k, hit[\"_source\"][k]) for k in hit[\"_source\"] if k != \"passage_text\"]) for hit in hits]\n",
    "    for r, hit in zip(res_list, hits):\n",
    "        r[\"passage_id\"] = hit[\"_id\"]\n",
    "        r[\"score\"] = hit[\"_score\"]\n",
    "        r[\"passage_text\"] = hit[\"_source\"][\"passage_text\"]\n",
    "    res_list = [res for res in res_list if len(res[\"passage_text\"].split()) > min_length][:n_results]\n",
    "    return support_doc, res_list\n",
    "\n",
    "\n",
    "def query_es_index(question, es_client, index_name=\"english_wiki_kilt_snippets_100w\", n_results=10, min_length=20):\n",
    "    q = question.lower()\n",
    "    banned = [\"how\", \"why\", \"what\", \"where\", \"which\", \"do\", \"does\", \"is\", \"?\", \"eli5\", \"eli5:\"]\n",
    "    q = \" \".join([w for w in q.split() if w not in banned])\n",
    "    response = es_client.search(\n",
    "        index=index_name,\n",
    "        body={\n",
    "            \"query\": {\n",
    "                \"multi_match\": {\n",
    "                    \"query\": q,\n",
    "                    \"fields\": [\"article_title\", \"section_title\", \"passage_text^2\"],\n",
    "                    \"type\": \"cross_fields\",\n",
    "                }\n",
    "            },\n",
    "            \"size\": 2 * n_results,\n",
    "        },\n",
    "    )\n",
    "    hits = response[\"hits\"][\"hits\"]\n",
    "    support_doc = \"<P> \" + \" <P> \".join([hit[\"_source\"][\"passage_text\"] for hit in hits])\n",
    "    res_list = [dict([(k, hit[\"_source\"][k]) for k in hit[\"_source\"] if k != \"passage_text\"]) for hit in hits]\n",
    "    for r, hit in zip(res_list, hits):\n",
    "        r[\"passage_id\"] = hit[\"_id\"]\n",
    "        r[\"score\"] = hit[\"_score\"]\n",
    "        r[\"passage_text\"] = hit[\"_source\"][\"passage_text\"]\n",
    "    res_list = [res for res in res_list if len(res[\"passage_text\"].split()) > min_length][:n_results]\n",
    "    return support_doc, res_list\n",
    "\n",
    "\n",
    "###############\n",
    "# ELI5 retriever training\n",
    "###############\n",
    "class ELI5DatasetQARetriver(Dataset):\n",
    "    def __init__(self, examples_array, extra_answer_threshold=3, min_answer_length=64, training=True, n_samples=None):\n",
    "        self.data = examples_array\n",
    "        self.answer_thres = extra_answer_threshold\n",
    "        self.min_length = min_answer_length\n",
    "        self.training = training\n",
    "        self.n_samples = self.data.num_rows if n_samples is None else n_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def make_example(self, idx):\n",
    "        example = self.data[idx]\n",
    "        question = example[\"title\"]\n",
    "        if self.training:\n",
    "            answers = [a for i, (a, sc) in enumerate(zip(example[\"answers\"][\"text\"], example[\"answers\"][\"score\"]))]\n",
    "            answer_tab = choice(answers).split(\" \")\n",
    "            start_idx = randint(0, max(0, len(answer_tab) - self.min_length))\n",
    "            answer_span = \" \".join(answer_tab[start_idx:])\n",
    "        else:\n",
    "            answer_span = example[\"answers\"][\"text\"][0]\n",
    "        return (question, answer_span)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.make_example(idx % self.data.num_rows)\n",
    "\n",
    "\n",
    "class RetrievalQAEmbedder(torch.nn.Module):\n",
    "    def __init__(self, sent_encoder, dim):\n",
    "        super(RetrievalQAEmbedder, self).__init__()\n",
    "        self.sent_encoder = sent_encoder\n",
    "        self.output_dim = 128\n",
    "        self.project_q = torch.nn.Linear(dim, self.output_dim, bias=False)\n",
    "        self.project_a = torch.nn.Linear(dim, self.output_dim, bias=False)\n",
    "        self.ce_loss = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "    def embed_sentences_checkpointed(self, input_ids, attention_mask, checkpoint_batch_size=-1):\n",
    "        # reproduces BERT forward pass with checkpointing\n",
    "        if checkpoint_batch_size < 0 or input_ids.shape[0] < checkpoint_batch_size:\n",
    "            return self.sent_encoder(input_ids, attention_mask=attention_mask)[1]\n",
    "        else:\n",
    "            # prepare implicit variables\n",
    "            device = input_ids.device\n",
    "            input_shape = input_ids.size()\n",
    "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
    "            head_mask = [None] * self.sent_encoder.config.num_hidden_layers\n",
    "            extended_attention_mask: torch.Tensor = self.sent_encoder.get_extended_attention_mask(\n",
    "                attention_mask, input_shape, device\n",
    "            )\n",
    "\n",
    "            # define function for checkpointing\n",
    "            def partial_encode(*inputs):\n",
    "                encoder_outputs = self.sent_encoder.encoder(inputs[0], attention_mask=inputs[1], head_mask=head_mask,)\n",
    "                sequence_output = encoder_outputs[0]\n",
    "                pooled_output = self.sent_encoder.pooler(sequence_output)\n",
    "                return pooled_output\n",
    "\n",
    "            # run embedding layer on everything at once\n",
    "            embedding_output = self.sent_encoder.embeddings(\n",
    "                input_ids=input_ids, position_ids=None, token_type_ids=token_type_ids, inputs_embeds=None\n",
    "            )\n",
    "            # run encoding and pooling on one mini-batch at a time\n",
    "            pooled_output_list = []\n",
    "            for b in range(math.ceil(input_ids.shape[0] / checkpoint_batch_size)):\n",
    "                b_embedding_output = embedding_output[b * checkpoint_batch_size : (b + 1) * checkpoint_batch_size]\n",
    "                b_attention_mask = extended_attention_mask[b * checkpoint_batch_size : (b + 1) * checkpoint_batch_size]\n",
    "                pooled_output = checkpoint.checkpoint(partial_encode, b_embedding_output, b_attention_mask)\n",
    "                pooled_output_list.append(pooled_output)\n",
    "            return torch.cat(pooled_output_list, dim=0)\n",
    "\n",
    "    def embed_questions(self, q_ids, q_mask, checkpoint_batch_size=-1):\n",
    "        q_reps = self.embed_sentences_checkpointed(q_ids, q_mask, checkpoint_batch_size)\n",
    "        return self.project_q(q_reps)\n",
    "\n",
    "    def embed_answers(self, a_ids, a_mask, checkpoint_batch_size=-1):\n",
    "        a_reps = self.embed_sentences_checkpointed(a_ids, a_mask, checkpoint_batch_size)\n",
    "        return self.project_a(a_reps)\n",
    "\n",
    "    def forward(self, q_ids, q_mask, a_ids, a_mask, checkpoint_batch_size=-1):\n",
    "        device = q_ids.device\n",
    "        q_reps = self.embed_questions(q_ids, q_mask, checkpoint_batch_size)\n",
    "        a_reps = self.embed_answers(a_ids, a_mask, checkpoint_batch_size)\n",
    "        compare_scores = torch.mm(q_reps, a_reps.t())\n",
    "        loss_qa = self.ce_loss(compare_scores, torch.arange(compare_scores.shape[1]).to(device))\n",
    "        loss_aq = self.ce_loss(compare_scores.t(), torch.arange(compare_scores.shape[0]).to(device))\n",
    "        loss = (loss_qa + loss_aq) / 2\n",
    "        return loss\n",
    "\n",
    "\n",
    "def make_qa_retriever_model(model_name=\"google/bert_uncased_L-8_H-512_A-8\", from_file=None, device=\"cuda:0\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    bert_model = AutoModel.from_pretrained(model_name).to(device)\n",
    "    # run bert_model on a dummy batch to get output dimension\n",
    "    d_ids = torch.LongTensor(\n",
    "        [[bert_model.config.bos_token_id if bert_model.config.bos_token_id is not None else 1]]\n",
    "    ).to(device)\n",
    "    d_mask = torch.LongTensor([[1]]).to(device)\n",
    "    sent_dim = bert_model(d_ids, attention_mask=d_mask)[1].shape[-1]\n",
    "    qa_embedder = RetrievalQAEmbedder(bert_model, sent_dim).to(device)\n",
    "    if from_file is not None:\n",
    "        param_dict = torch.load(from_file)  # has model weights, optimizer, and scheduler states\n",
    "        qa_embedder.load_state_dict(param_dict[\"model\"])\n",
    "    return tokenizer, qa_embedder\n",
    "\n",
    "\n",
    "def make_qa_retriever_batch(qa_list, tokenizer, max_len=64, device=\"cuda:0\"):\n",
    "    q_ls = [q for q, a in qa_list]\n",
    "    a_ls = [a for q, a in qa_list]\n",
    "    q_toks = tokenizer.batch_encode_plus(q_ls, max_length=max_len, pad_to_max_length=True, truncation=True)\n",
    "    q_ids, q_mask = (\n",
    "        torch.LongTensor(q_toks[\"input_ids\"]).to(device),\n",
    "        torch.LongTensor(q_toks[\"attention_mask\"]).to(device),\n",
    "    )\n",
    "    a_toks = tokenizer.batch_encode_plus(a_ls, max_length=max_len, pad_to_max_length=True, truncation=True)\n",
    "    a_ids, a_mask = (\n",
    "        torch.LongTensor(a_toks[\"input_ids\"]).to(device),\n",
    "        torch.LongTensor(a_toks[\"attention_mask\"]).to(device),\n",
    "    )\n",
    "    return (q_ids, q_mask, a_ids, a_mask)\n",
    "\n",
    "\n",
    "def train_qa_retriever_epoch(model, dataset, tokenizer, optimizer, scheduler, args, e=0):\n",
    "    model.train()\n",
    "    # make iterator\n",
    "    train_sampler = RandomSampler(dataset)\n",
    "    model_collate_fn = functools.partial(\n",
    "        make_qa_retriever_batch, tokenizer=tokenizer, max_len=args.max_length, device=\"cuda:0\"\n",
    "    )\n",
    "    data_loader = DataLoader(dataset, batch_size=args.batch_size, sampler=train_sampler, collate_fn=model_collate_fn)\n",
    "    epoch_iterator = tqdm(data_loader, desc=\"Iteration\", disable=True)\n",
    "    # accumulate loss since last print\n",
    "    loc_steps = 0\n",
    "    loc_loss = 0.0\n",
    "    st_time = time()\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        q_ids, q_mask, a_ids, a_mask = batch\n",
    "        pre_loss = model(q_ids, q_mask, a_ids, a_mask, checkpoint_batch_size=args.checkpoint_batch_size)\n",
    "        loss = pre_loss.sum()\n",
    "        # optimizer\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        model.zero_grad()\n",
    "        # some printing within the epoch\n",
    "        loc_loss += loss.item()\n",
    "        loc_steps += 1\n",
    "        if step % args.print_freq == 0 or step == 1:\n",
    "            print(\n",
    "                \"{:2d} {:5d} of {:5d} \\t L: {:.3f} \\t -- {:.3f}\".format(\n",
    "                    e, step, len(dataset) // args.batch_size, loc_loss / loc_steps, time() - st_time,\n",
    "                )\n",
    "            )\n",
    "            loc_loss = 0\n",
    "            loc_steps = 0\n",
    "\n",
    "\n",
    "def train_qa_retriever_joint_epoch(model, dataset_list, tokenizer, optimizer, scheduler, args, e=0):\n",
    "    model.train()\n",
    "    model_collate_fn = functools.partial(\n",
    "        make_qa_retriever_batch, tokenizer=tokenizer, max_len=args.max_length, device=\"cuda:0\"\n",
    "    )\n",
    "    # make iterator\n",
    "    train_samplers = [RandomSampler(dataset) for dataset in dataset_list]\n",
    "    data_loaders = [\n",
    "        DataLoader(dataset, batch_size=args.batch_size, sampler=train_sampler, collate_fn=model_collate_fn)\n",
    "        for dataset, train_sampler in zip(dataset_list, train_samplers)\n",
    "    ]\n",
    "    iterators = [iter(dloader) for dloader in data_loaders]\n",
    "    joint_iter = zip(*iterators)\n",
    "    # accumulate loss since last print\n",
    "    loc_steps = 0\n",
    "    loc_loss = 0.0\n",
    "    st_time = time()\n",
    "    for step, (batches,) in enumerate(zip(joint_iter)):\n",
    "        for batch in batches:\n",
    "            q_ids, q_mask, a_ids, a_mask = batch\n",
    "            loss = model(q_ids, q_mask, a_ids, a_mask, checkpoint_batch_size=args.checkpoint_batch_size)\n",
    "            # optimizer\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            model.zero_grad()\n",
    "            # some printing within the epoch\n",
    "            loc_loss += loss.item()\n",
    "            loc_steps += 1\n",
    "        if step % args.print_freq == 0:\n",
    "            print(\n",
    "                \"{:2d} {:5d} of {:5d} \\t L: {:.3f} \\t -- {:.3f}\".format(\n",
    "                    e, step, len(dataset_list[0]) // args.batch_size, loc_loss / loc_steps, time() - st_time,\n",
    "                )\n",
    "            )\n",
    "            loc_loss = 0\n",
    "            loc_steps = 0\n",
    "\n",
    "\n",
    "def evaluate_qa_retriever(model, dataset, tokenizer, args):\n",
    "    model.eval()\n",
    "    # make iterator\n",
    "    eval_sampler = SequentialSampler(dataset)\n",
    "    model_collate_fn = functools.partial(\n",
    "        make_qa_retriever_batch, tokenizer=tokenizer, max_len=args.max_length, device=\"cuda:0\"\n",
    "    )\n",
    "    data_loader = DataLoader(dataset, batch_size=args.batch_size, sampler=eval_sampler, collate_fn=model_collate_fn)\n",
    "    epoch_iterator = tqdm(data_loader, desc=\"Iteration\", disable=True)\n",
    "    tot_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            q_ids, q_mask, a_ids, a_mask = batch\n",
    "            loss = model(q_ids, q_mask, a_ids, a_mask)\n",
    "            tot_loss += loss.item()\n",
    "        return tot_loss / (step + 1)\n",
    "\n",
    "\n",
    "def train_qa_retriever(qar_model, qar_tokenizer, qar_train_dset, qar_valid_dset, qar_args):\n",
    "    qar_optimizer = AdamW(qar_model.parameters(), lr=qar_args.learning_rate, eps=1e-8)\n",
    "    qar_scheduler = get_linear_schedule_with_warmup(\n",
    "        qar_optimizer,\n",
    "        num_warmup_steps=100,\n",
    "        num_training_steps=(qar_args.num_epochs + 1) * math.ceil(len(qar_train_dset) / qar_args.batch_size),\n",
    "    )\n",
    "    for e in range(qar_args.num_epochs):\n",
    "        train_qa_retriever_epoch(qar_model, qar_train_dset, qar_tokenizer, qar_optimizer, qar_scheduler, qar_args, e)\n",
    "        m_save_dict = {\n",
    "            \"model\": qar_model.state_dict(),\n",
    "            \"optimizer\": qar_optimizer.state_dict(),\n",
    "            \"scheduler\": qar_scheduler.state_dict(),\n",
    "        }\n",
    "        print(\"Saving model {}\".format(qar_args.model_save_name))\n",
    "        torch.save(m_save_dict, \"{}_{}.pth\".format(qar_args.model_save_name, e))\n",
    "        eval_loss = evaluate_qa_retriever(qar_model, qar_valid_dset, qar_tokenizer, qar_args)\n",
    "        print(\"Evaluation loss epoch {:4d}: {:.3f}\".format(e, eval_loss))\n",
    "\n",
    "\n",
    "###############\n",
    "# ELI5 seq2seq model training\n",
    "###############\n",
    "class ELI5DatasetS2S(Dataset):\n",
    "    def __init__(\n",
    "        self, examples_array, make_doc_fun=None, extra_answer_threshold=3, document_cache=None, training=True\n",
    "    ):\n",
    "        self.training = training\n",
    "        self.data = examples_array\n",
    "        self.make_doc_function = make_doc_fun\n",
    "        self.document_cache = {} if document_cache is None else document_cache\n",
    "        assert not (make_doc_fun is None and document_cache is None)\n",
    "        # make index of specific question-answer pairs from multi-answers\n",
    "        if self.training:\n",
    "            self.qa_id_list = [\n",
    "                (i, j)\n",
    "                for i, qa in enumerate(self.data)\n",
    "                for j, (a, sc) in enumerate(zip(qa[\"answers\"][\"text\"], qa[\"answers\"][\"score\"]))\n",
    "                if j == 0 or sc >= extra_answer_threshold\n",
    "            ]\n",
    "        else:\n",
    "            self.qa_id_list = [(i, 0) for i in range(self.data.num_rows)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.qa_id_list)\n",
    "\n",
    "    def make_example(self, idx):\n",
    "        i, j = self.qa_id_list[idx]\n",
    "        example = self.data[i]\n",
    "        question = example[\"title\"] + \" \" + example[\"selftext\"]\n",
    "        answer = example[\"answers\"][\"text\"][j]\n",
    "        q_id = example[\"q_id\"]\n",
    "        if self.make_doc_function is not None:\n",
    "            self.document_cache[q_id] = self.document_cache.get(q_id, self.make_doc_function(example[\"title\"]))\n",
    "        document = self.document_cache[q_id]\n",
    "        in_st = \"question: {} context: {}\".format(\n",
    "            question.lower().replace(\" --t--\", \"\").strip(), document.lower().strip(),\n",
    "        )\n",
    "        out_st = answer\n",
    "        return (in_st, out_st)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.make_example(idx)\n",
    "\n",
    "\n",
    "def make_qa_s2s_model(model_name=\"facebook/bart-large\", from_file=None, device=\"cuda:0\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "    if from_file is not None:\n",
    "        param_dict = torch.load(from_file)  # has model weights, optimizer, and scheduler states\n",
    "        model.load_state_dict(param_dict[\"model\"])\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "def make_qa_s2s_batch(qa_list, tokenizer, max_len=64, max_a_len=360, device=\"cuda:0\"):\n",
    "    q_ls = [q for q, a in qa_list]\n",
    "    a_ls = [a for q, a in qa_list]\n",
    "    q_toks = tokenizer.batch_encode_plus(q_ls, max_length=max_len, pad_to_max_length=True, truncation=True)\n",
    "    q_ids, q_mask = (\n",
    "        torch.LongTensor(q_toks[\"input_ids\"]).to(device),\n",
    "        torch.LongTensor(q_toks[\"attention_mask\"]).to(device),\n",
    "    )\n",
    "    a_toks = tokenizer.batch_encode_plus(a_ls, max_length=min(max_len, max_a_len), pad_to_max_length=True, truncation=True)\n",
    "    a_ids, a_mask = (\n",
    "        torch.LongTensor(a_toks[\"input_ids\"]).to(device),\n",
    "        torch.LongTensor(a_toks[\"attention_mask\"]).to(device),\n",
    "    )\n",
    "    lm_labels = a_ids[:, 1:].contiguous().clone()\n",
    "    lm_labels[a_mask[:, 1:].contiguous() == 0] = -100\n",
    "    model_inputs = {\n",
    "        \"input_ids\": q_ids,\n",
    "        \"attention_mask\": q_mask,\n",
    "        \"decoder_input_ids\": a_ids[:, :-1].contiguous(),\n",
    "        \"lm_labels\": lm_labels,\n",
    "    }\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def train_qa_s2s_epoch(model, dataset, tokenizer, optimizer, scheduler, args, e=0, curriculum=False):\n",
    "    model.train()\n",
    "    # make iterator\n",
    "    if curriculum:\n",
    "        train_sampler = SequentialSampler(dataset)\n",
    "    else:\n",
    "        train_sampler = RandomSampler(dataset)\n",
    "    model_collate_fn = functools.partial(\n",
    "        make_qa_s2s_batch, tokenizer=tokenizer, max_len=args.max_length, device=\"cuda:0\"\n",
    "    )\n",
    "    data_loader = DataLoader(dataset, batch_size=args.batch_size, sampler=train_sampler, collate_fn=model_collate_fn)\n",
    "    epoch_iterator = tqdm(data_loader, desc=\"Iteration\", disable=True)\n",
    "    # accumulate loss since last print\n",
    "    loc_steps = 0\n",
    "    loc_loss = 0.0\n",
    "    st_time = time()\n",
    "    for step, batch_inputs in enumerate(epoch_iterator):\n",
    "        pre_loss = model(**batch_inputs)[0]\n",
    "        loss = pre_loss.sum() / pre_loss.shape[0]\n",
    "        loss.backward()\n",
    "        # optimizer\n",
    "        if step % args.backward_freq == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            model.zero_grad()\n",
    "        # some printing within the epoch\n",
    "        loc_loss += loss.item()\n",
    "        loc_steps += 1\n",
    "        if step % args.print_freq == 0 or step == 1:\n",
    "            print(\n",
    "                \"{:2d} {:5d} of {:5d} \\t L: {:.3f} \\t -- {:.3f}\".format(\n",
    "                    e, step, len(dataset) // args.batch_size, loc_loss / loc_steps, time() - st_time,\n",
    "                )\n",
    "            )\n",
    "            loc_loss = 0\n",
    "            loc_steps = 0\n",
    "\n",
    "\n",
    "def eval_qa_s2s_epoch(model, dataset, tokenizer, args):\n",
    "    model.eval()\n",
    "    # make iterator\n",
    "    train_sampler = SequentialSampler(dataset)\n",
    "    model_collate_fn = functools.partial(\n",
    "        make_qa_s2s_batch, tokenizer=tokenizer, max_len=args.max_length, device=\"cuda:0\"\n",
    "    )\n",
    "    data_loader = DataLoader(dataset, batch_size=args.batch_size, sampler=train_sampler, collate_fn=model_collate_fn)\n",
    "    epoch_iterator = tqdm(data_loader, desc=\"Iteration\", disable=True)\n",
    "    # accumulate loss since last print\n",
    "    loc_steps = 0\n",
    "    loc_loss = 0.0\n",
    "    st_time = time()\n",
    "    with torch.no_grad():\n",
    "        for step, batch_inputs in enumerate(epoch_iterator):\n",
    "            pre_loss = model(**batch_inputs)[0]\n",
    "            loss = pre_loss.sum() / pre_loss.shape[0]\n",
    "            loc_loss += loss.item()\n",
    "            loc_steps += 1\n",
    "            if step % args.print_freq == 0:\n",
    "                print(\n",
    "                    \"{:5d} of {:5d} \\t L: {:.3f} \\t -- {:.3f}\".format(\n",
    "                        step, len(dataset) // args.batch_size, loc_loss / loc_steps, time() - st_time,\n",
    "                    )\n",
    "                )\n",
    "    print(\"Total \\t L: {:.3f} \\t -- {:.3f}\".format(loc_loss / loc_steps, time() - st_time,))\n",
    "\n",
    "\n",
    "def train_qa_s2s(qa_s2s_model, qa_s2s_tokenizer, s2s_train_dset, s2s_valid_dset, s2s_args):\n",
    "    s2s_optimizer = AdamW(qa_s2s_model.parameters(), lr=s2s_args.learning_rate, eps=1e-8)\n",
    "    s2s_scheduler = get_linear_schedule_with_warmup(\n",
    "        s2s_optimizer,\n",
    "        num_warmup_steps=400,\n",
    "        num_training_steps=(s2s_args.num_epochs + 1) * math.ceil(len(s2s_train_dset) / s2s_args.batch_size),\n",
    "    )\n",
    "    for e in range(s2s_args.num_epochs):\n",
    "        train_qa_s2s_epoch(\n",
    "            qa_s2s_model,\n",
    "            s2s_train_dset,\n",
    "            qa_s2s_tokenizer,\n",
    "            s2s_optimizer,\n",
    "            s2s_scheduler,\n",
    "            s2s_args,\n",
    "            e,\n",
    "            curriculum=(e == 0),\n",
    "        )\n",
    "        m_save_dict = {\n",
    "            \"model\": qa_s2s_model.state_dict(),\n",
    "            \"optimizer\": s2s_optimizer.state_dict(),\n",
    "            \"scheduler\": s2s_scheduler.state_dict(),\n",
    "        }\n",
    "        print(\"Saving model {}\".format(s2s_args.model_save_name))\n",
    "        eval_qa_s2s_epoch(qa_s2s_model, s2s_valid_dset, qa_s2s_tokenizer, s2s_args)\n",
    "        torch.save(m_save_dict, \"{}_{}.pth\".format(s2s_args.model_save_name, e))\n",
    "\n",
    "\n",
    "# generate answer from input \"question: ... context: <p> ...\"\n",
    "def qa_s2s_generate(\n",
    "    question_doc,\n",
    "    qa_s2s_model,\n",
    "    qa_s2s_tokenizer,\n",
    "    num_answers=1,\n",
    "    num_beams=None,\n",
    "    min_len=64,\n",
    "    max_len=256,\n",
    "    do_sample=False,\n",
    "    temp=1.0,\n",
    "    top_p=None,\n",
    "    top_k=None,\n",
    "    max_input_length=512,\n",
    "    device=\"cuda:0\",\n",
    "):\n",
    "    model_inputs = make_qa_s2s_batch([(question_doc, \"A\")], qa_s2s_tokenizer, max_input_length, device=device,)\n",
    "    n_beams = num_answers if num_beams is None else max(num_beams, num_answers)\n",
    "    generated_ids = qa_s2s_model.generate(\n",
    "        input_ids=model_inputs[\"input_ids\"],\n",
    "        attention_mask=model_inputs[\"attention_mask\"],\n",
    "        min_length=min_len,\n",
    "        max_length=max_len,\n",
    "        do_sample=do_sample,\n",
    "        early_stopping=True,\n",
    "        num_beams=1 if do_sample else n_beams,\n",
    "        temperature=temp,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        eos_token_id=qa_s2s_tokenizer.eos_token_id,\n",
    "        no_repeat_ngram_size=3,\n",
    "        num_return_sequences=num_answers,\n",
    "        decoder_start_token_id=qa_s2s_tokenizer.bos_token_id,\n",
    "    )\n",
    "    return [qa_s2s_tokenizer.decode(ans_ids, skip_special_tokens=True).strip() for ans_ids in generated_ids]\n",
    "\n",
    "\n",
    "###############\n",
    "# ELI5-trained retrieval model usage\n",
    "###############\n",
    "def embed_passages_for_retrieval(passages, tokenizer, qa_embedder, max_length=128, device=\"cuda:0\"):\n",
    "    a_toks = tokenizer.batch_encode_plus(passages, max_length=max_length, pad_to_max_length=True, truncation=True)\n",
    "    a_ids, a_mask = (\n",
    "        torch.LongTensor(a_toks[\"input_ids\"]).to(device),\n",
    "        torch.LongTensor(a_toks[\"attention_mask\"]).to(device),\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        a_reps = qa_embedder.embed_answers(a_ids, a_mask).cpu().type(torch.float)\n",
    "    return a_reps.numpy()\n",
    "\n",
    "\n",
    "def embed_questions_for_retrieval(q_ls, tokenizer, qa_embedder, device=\"cuda:0\"):\n",
    "    q_toks = tokenizer.batch_encode_plus(q_ls, max_length=128, pad_to_max_length=True, truncation=True)\n",
    "    q_ids, q_mask = (\n",
    "        torch.LongTensor(q_toks[\"input_ids\"]).to(device),\n",
    "        torch.LongTensor(q_toks[\"attention_mask\"]).to(device),\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        q_reps = qa_embedder.embed_questions(q_ids, q_mask).cpu().type(torch.float)\n",
    "    return q_reps.numpy()\n",
    "\n",
    "\n",
    "def make_qa_dense_index(\n",
    "    qa_embedder,\n",
    "    tokenizer,\n",
    "    passages_dset,\n",
    "    batch_size=512,\n",
    "    max_length=128,\n",
    "    index_name=\"kilt_passages_reps.dat\",\n",
    "    dtype=\"float32\",\n",
    "    device=\"cuda:0\",\n",
    "):\n",
    "    st_time = time()\n",
    "    fp = np.memmap(index_name, dtype=dtype, mode=\"w+\", shape=(passages_dset.num_rows, 128))\n",
    "    n_batches = math.ceil(passages_dset.num_rows / batch_size)\n",
    "    print(\"Number of batches: {}\".format(n_batches))\n",
    "    for i in range(n_batches):\n",
    "        passages = [p for p in passages_dset[i * batch_size : (i + 1) * batch_size][\"passage_text\"]]\n",
    "        reps = embed_passages_for_retrieval(passages, tokenizer, qa_embedder, max_length, device)\n",
    "        fp[i * batch_size : (i + 1) * batch_size] = reps\n",
    "        if i % 50 == 0:\n",
    "            print(i, time() - st_time)\n",
    "\n",
    "\n",
    "def evaluate_retriever(qa_list, retriever_func, scoring_func, n_ret=10, verbose=False):\n",
    "    total_retriever_time = 0.0\n",
    "    total_retriever_score = 0.0\n",
    "    st_time = time()\n",
    "    for i, (question, answer) in enumerate(qa_list):\n",
    "        r_time = time()\n",
    "        retrieved_passages = retriever_func(question, n_ret)\n",
    "        total_retriever_time += time() - r_time\n",
    "        total_retriever_score += scoring_func(retrieved_passages, answer)\n",
    "        if verbose and ((i + 1) % 500 == 0 or i <= 1):\n",
    "            print(\n",
    "                \"{:03d}: S-{:.4f} T-{:.4f} | {:.2f}\".format(\n",
    "                    i + 1, total_retriever_score / (i + 1), total_retriever_time / (i + 1), time() - st_time\n",
    "                )\n",
    "            )\n",
    "    return {\"idf_recall\": total_retriever_score / (i + 1), \"retrieval_time\": total_retriever_time / (i + 1)}\n",
    "\n",
    "\n",
    "# build a support document for the question out of Wikipedia snippets\n",
    "def query_qa_dense_index(\n",
    "    question, qa_embedder, tokenizer, wiki_passages, wiki_index, n_results=10, min_length=20, device=\"cuda:0\"\n",
    "):\n",
    "    q_rep = embed_questions_for_retrieval([question], tokenizer, qa_embedder, device=device)\n",
    "    D, I = wiki_index.search(q_rep, 2 * n_results)\n",
    "    res_passages = [wiki_passages[int(i)] for i in I[0]]\n",
    "    support_doc = \"<P> \" + \" <P> \".join([p[\"passage_text\"] for p in res_passages])\n",
    "    res_list = [dict([(k, p[k]) for k in wiki_passages.column_names]) for p in res_passages]\n",
    "    res_list = [res for res in res_list if len(res[\"passage_text\"].split()) > min_length][:n_results]\n",
    "    for r, sc in zip(res_list, D[0]):\n",
    "        r[\"score\"] = float(sc)\n",
    "    return support_doc, res_list\n",
    "\n",
    "\n",
    "def batch_query_qa_dense_index(questions, qa_embedder, tokenizer, wiki_passages, wiki_index, n_results=10):\n",
    "    q_rep = embed_questions_for_retrieval(questions, tokenizer, qa_embedder)\n",
    "    D, I = wiki_index.search(q_rep, n_results)\n",
    "    res_passages_lst = [[wiki_passages[int(i)] for i in i_lst] for i_lst in I]\n",
    "    support_doc_lst = [\n",
    "        \"<P> \" + \" <P> \".join([p[\"passage_text\"] for p in res_passages]) for res_passages in res_passages_lst\n",
    "    ]\n",
    "    all_res_lists = []\n",
    "    for (res_passages, dl) in zip(res_passages_lst, D):\n",
    "        res_list = [dict([(k, p[k]) for k in wiki_passages.column_names]) for p in res_passages]\n",
    "        for r, sc in zip(res_list, dl):\n",
    "            r[\"score\"] = float(sc)\n",
    "        all_res_lists += [res_list[:]]\n",
    "    return support_doc_lst, all_res_lists\n",
    "\n",
    "\n",
    "# find nearest neighbors of an answer or declarative text in Wikipedia snippets\n",
    "def query_qa_dense_index_nn(passage, qa_embedder, tokenizer, wiki_passages, wiki_index, n_results=10, min_length=20):\n",
    "    a_rep = embed_passages_for_retrieval([passage], tokenizer, qa_embedder)\n",
    "    D, I = wiki_index.search(a_rep, 2 * n_results)\n",
    "    res_passages = [wiki_passages[int(i)] for i in I[0]]\n",
    "    support_doc = \"<P> \" + \" <P> \".join([p[\"passage_text\"] for p in res_passages])\n",
    "    res_list = [dict([(k, p[k]) for k in wiki_passages.column_names]) for p in res_passages]\n",
    "    res_list = [res for res in res_list if len(res[\"passage_text\"].split()) > min_length][:n_results]\n",
    "    for r, sc, i in zip(res_list, D[0], I[0]):\n",
    "        r[\"passage_id\"] = int(i)\n",
    "        r[\"score\"] = float(sc)\n",
    "    return support_doc, res_list\n",
    "\n",
    "\n",
    "def batch_query_qa_dense_index_nn(passages, qa_embedder, tokenizer, wiki_passages, wiki_index, n_results=10):\n",
    "    a_reps = embed_passages_for_retrieval(passages, tokenizer, qa_embedder)\n",
    "    D, I = wiki_index.search(a_reps, n_results)\n",
    "    res_passages_lst = [[wiki_passages[int(i)] for i in i_lst] for i_lst in I]\n",
    "    support_doc_lst = [\n",
    "        \"<P> \" + \" <P> \".join([p[\"passage_text\"] for p in res_passages]) for res_passages in res_passages_lst\n",
    "    ]\n",
    "    all_res_lists = []\n",
    "    for (res_passages, dl, il) in zip(res_passages_lst, D, I):\n",
    "        res_list = [dict([(k, p[k]) for k in wiki_passages.column_names]) for p in res_passages]\n",
    "        for r, sc, i in zip(res_list, dl, il):\n",
    "            r[\"passage_id\"] = int(i)\n",
    "            r[\"score\"] = float(sc)\n",
    "        all_res_lists += [res_list[:]]\n",
    "    return support_doc_lst, all_res_lists\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Python3"
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "bash"
   },
   "source": [
    "Set up elastic search client. Docker process must be running!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "kernel": "Python3"
   },
   "outputs": [],
   "source": [
    "es_client = Elasticsearch([{'host': 'localhost', 'port': '9200'}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Python3"
   },
   "source": [
    "**Load Wikipedia data for long form question answering / elaborated feedback**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "kernel": "Python3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17553713/17553713 [2:34:06<00:00, 1898.32docs/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 17553713 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if not es_client.indices.exists('wiki40b_snippets_100w'):\n",
    "    make_es_index_snippets(es_client, wiki40b_snippets, index_name='wiki40b_snippets_100w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Python3"
   },
   "source": [
    "**OPTIONAL - if you have a custom data source, you can load it by modifying the steps below**\n",
    "\n",
    "- Clean your text (as though a human will read it)\n",
    "- Concat to single string\n",
    "- Window into 100 word sections (not overlapping)\n",
    "- Output a file in the required format, which appears to be an array of objects that look like this, however, only 'article_title', 'section_title', and 'passage_text' seem to be used:\n",
    "\n",
    "```json\n",
    "{'_id': '{\"nlp_id\": 1665419, \"wiki_id\": \"Q179635\", \"sp\": 12, \"sc\": 65\n",
    "3, \"ep\": 12, \"ec\": 1223}',\n",
    "'nlp_id': 1665419,\n",
    "'wiki_id': 'Q179635',\n",
    "'start_paragraph': 12,\n",
    "'start_character': 653,\n",
    "'end_paragraph': 12,\n",
    "'end_character': 1223,\n",
    "'article_title': 'Heat transfer',\n",
    "'section_title': 'Conduction',\n",
    "'passage_text': 'from one place to another place without the movemen\n",
    "t of particles is called conduction, such as when placing a hand on a\n",
    "cold glass of water - heat is conducted from the warm skin to the col\n",
    "d glass, but if the hand is held a few inches from the glass, little\n",
    "conduction would occur since air is a poor conductor of heat. Steady\n",
    "state conduction is an idealized model of conduction that happens whe\n",
    "n the temperature difference driving the conduction is constant, so t\n",
    "hat after a time, the spatial distribution of temperatures in the con\n",
    "ducting object does not change any'}\n",
    "```\n",
    "- Use the code below to load the resulting `json` file into elastic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "kernel": "Python3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2090/2090 [00:00<00:00, 5748.51docs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 2090 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Change the file path and the name of your data source to suit. The name of the data source (AKA index) will be used when you query\n",
    "import json\n",
    "with open(\"/z/aolney/research_projects/mofacts/materials/2019-09-28-holes-ap-book-cloze/wes-download-html/ap-eli5-wiki-format.json\",\"r\") as f:\n",
    "    ap_snippets = json.loads( f.read() )\n",
    "    if not es_client.indices.exists('ap_snippets_100w'):\n",
    "        make_es_index_snippets_textbook(es_client, ap_snippets, index_name='ap_snippets_100w')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "kernel": "Python3"
   },
   "outputs": [],
   "source": [
    "# using docker API\n",
    "# import requests\n",
    "# with open(\"/z/aolney/research_projects/mofacts/materials/ap-book/wes-download-html/ap-eli5-wiki-format.json\",\"r\") as f:\n",
    "#     snippetJson = f.read()\n",
    "#     r = requests.post('http://localhost:8005', json={\"snippets\": snippetJson})\n",
    "#     print(r.status_code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "sos": {
   "kernels": [
    [
     "Python3",
     "python3",
     "Python3",
     "#FFD91A",
     {
      "name": "ipython",
      "version": 3
     }
    ],
    [
     "bash",
     "bash",
     "bash",
     "",
     "shell"
    ]
   ],
   "version": "0.20.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
